answer,isPublic,question
"In the clinical domain, experts are often challenged to find condensed but accurate information from dispersed and heterogeneous clinical notes. AI models can alleviate this by automatically retrieving it based on the semantic representation, and therefore it is critical that the model can accurately represent the meaning of each sentence. However, high-quality clinical sentence embedding yet remains a challenging objective due to 1) heterogeneity and redundancy in clinical notes. 2) medical jargons 3) sentences often have similar structures with the only difference being the complex medical concepts being discussed. MED-SE is a new training scheme specialized in the clinical domain to overcome these challenges.",True,Why do we need MED-SE? 
"MED-SE is a contrastive-learning based framework consisting of two training tasks. 1) Sentence-level loss (SL), where we form positive pair by relying on the dropout scheme of the AI model along with the in-batch negatives. 2) Entity-level loss (EL), where we extract the word embeddings from 1) and form a contrastive-learning scheme with its corresponding definitions. Our results show best performance when overall loss is formed by SL + 0.1 * EL.",True,How does MED-SE work?
"We train on MIMIC-III discharge summaries, and test on the Clinical STS dataset with zero-shot setting. While performance is similar when the training (fine-tuning) corpus consists of S_all (union of sentences w/ & w/o medical jargons): 74.2 (SimCSE) vs 73.7 (ours). when the training corpus consists of S_ent (sentences w/ medical jargons), the performance is much higher for our framework: 56.0 (SimCSE) vs 72.8 (ours). Meanwhile, we also show that to achieve highest performance, it is critical to both utilize sentences w/ medical jargons and w/o medical jargon, and MED-SE can help reinforce the learning process of the combined two sets.",True,"How was MED-SE evaluated, and what is the evaluation result?"
